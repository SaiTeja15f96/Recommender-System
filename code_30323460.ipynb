{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignment 2 for FIT5212, Semester 1, 2020\n",
    "\n",
    "**Student Name:**  SAI TEJA POTHNAK\n",
    "\n",
    "**Student ID:**    30323460"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Task 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Recommender Systems"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Collaborative Filtering\n",
    "This is a technique to filter out the products that customer might prefer based on the behaviour by similar customers.When we talk about collaborative filtering for recommender systems we want to solve the problem of our original matrix having million dimensions.\n",
    "\n",
    "Instead we can use matrix factorisation to mathematically reduce dimensionality of original matrix into smaller matrix i.e. All users by all items into all items by some taste dimensions and all users into some taste dimensions.\n",
    "\n",
    "If we could represent user as a vector of their taste value and at the same time represent item as a vector of their taste value, we can quite easily make a recom/mendation.\n",
    "\n",
    "This can also help us find connections between users who have no specific items in common but share common tastes.\n",
    "\n",
    "\n",
    "## Implicit Data\n",
    "Implicit Data is the type of feedback data that is generated depending on user's behaviour with no specific actions from the user. For example, how many times a user has visited a site, time spent by the user on particular product, items that are purchased by the user. This kind of data is gathered through user interaction and contains lots of data. The downside is that most of this data is noisy and not always apparent what it means."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "hdTutLs7Eipu"
   },
   "outputs": [],
   "source": [
    "#importing libraries for the task\n",
    "import implicit\n",
    "from sklearn import metrics\n",
    "import scipy.sparse as sparse\n",
    "from scipy.sparse import csr_matrix\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import itertools\n",
    "from itertools import chain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* About the Data Sets :\n",
    "\n",
    " \n",
    " 1. `Train Dataset`: It contains set of interactions between user id and item id. i.e. if a user interacts with an item, there will be record as rating in the dataset.\n",
    "    \n",
    "    \n",
    "    \n",
    " 2. `Test Dataset` : This dataset contains list of users where each user is provided with a list with 100 item candidates. This is later used for comparision after generating recommended items from the model and final top 10 items are recommended.\n",
    "    \n",
    "    \n",
    "    \n",
    " 3. `Validation Dataset`  : This dataset is similar to Test dataset and is concatenated with train dataset and used for model construction and tuning."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "fOwdbQ7WEip3"
   },
   "outputs": [],
   "source": [
    "trainData =pd.read_csv('train_data.csv') #reading train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nEMJawz_Eip7"
   },
   "outputs": [],
   "source": [
    "validationData=pd.read_csv('validation_data.csv') #reading test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "testData=pd.read_csv('test_data.csv') #reading the test data csv file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "PyzDI8QXLA8X",
    "outputId": "a9393280-3559-4a5d-ab3e-82415a537f3e"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28449, 3)"
      ]
     },
     "execution_count": 45,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.shape #shape of training dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "JNOTE5x8LA8Z"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223900, 3)"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationData.shape #shape of validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(223900, 2)"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.shape #shape of test dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "2Dps3hGiEip-",
    "outputId": "197c1513-3927-486c-9925-f63b8414dceb",
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>3</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>4</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        0        0       1\n",
       "1        0        1       1\n",
       "2        0        2       1\n",
       "3        0        3       1\n",
       "4        0        4       1"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "trainData.head() #looking at the first 5 rows of train data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 195
    },
    "colab_type": "code",
    "id": "-h9p0621EiqC",
    "outputId": "fd429ff2-83d5-46d9-83f0-076a3baf4db5"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "      <th>rating</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>43</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>1102</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>815</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>739</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1637</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id  rating\n",
       "0        0       43       1\n",
       "1        0     1102       0\n",
       "2        0      815       0\n",
       "3        0      739       0\n",
       "4        0     1637       0"
      ]
     },
     "execution_count": 49,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "validationData.head() #looking at first 5 rows of validation data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>2158</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>2113</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>2070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>2026</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1948</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   user_id  item_id\n",
       "0        0     2158\n",
       "1        0     2113\n",
       "2        0     2070\n",
       "3        0     2026\n",
       "4        0     1948"
      ]
     },
     "execution_count": 50,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testData.head() #looking at first 5 rows of test data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Alternating Least Squares\n",
    "\n",
    "Alternating Least Squares (ALS) is a recommendation model we will use to fit our data and recommend items to each user.\n",
    "\n",
    "**Methodology :**\n",
    "In Als, optimization process is performed for  every iteration till the matrix arrives closer to factorized representation of original data.\n",
    "\n",
    "Initially, random values for users(U) and items(I) are assigned and using least squares method iteratively such that say in first instance it optimises U and fixes V and vice versa until both weights are optimised to reach the best approximation of original matrix. It also contains a regulariser term 'lambda' to reduce overfitting.\n",
    "\n",
    "This algorithm minimises the loss functona and finds out the optimised user and item vectors.\n",
    "Hence, by performing alternative iteration and optimisation process we generate one matrix with user vectors and one with item vectors that can be later used find similarities and also suggest item recommendations to the user.\n",
    "\n",
    "**Parameters :**\n",
    "1. **Confidence :**  It is calculated using magnitude of the feedback. More the user spent time with the product more is the confidence.\n",
    "2. Confidence is calculated by `c = 1 + alpha * r ( where alpha is rate at which the confidence increases and r is the rating information)`\n",
    "3. `lambda` is the regularisation term as discussed earlier\n",
    "4. `factors` is the number of latent factors to compute the model\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Personalized Ranking\n",
    "It provides user with ranked list of items. In this process, instead of taking single item as training data, it takes pair of items. The optimization is performed on rank of the user. For example, let's say there are two items i1, i2. user interacted with i1 and not with i2. This approach gives i1 a positive sign anf prefers item i1 over i2. Moreover,bayesian approach maximises the posterior probability for each user. It assumes the user interacted item pair independent of every other item pair.\n",
    "\n",
    "User specific log likelihood function is given with following assumptions:\n",
    "1. Users are independent to each other.\n",
    "2. Item pair association is independent to one another."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model Development \n",
    "\n",
    "### Three Models are used and compared here :\n",
    "\n",
    "##### 1. Alternative Least Squared with only Training Data\n",
    "#### 2. Alternative Least Squares with both Training and Validation Data#### \n",
    "#### 3. Bayesian Personalized Ranking method with both Training and Validataion Dataset\n",
    "\n",
    "## Model 1 :Fitting the ALS Model with only Training Data:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "data1=trainData #storing training data into a new variable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(28449, 3)"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data1.shape #shape of final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "users1=list(np.sort(data1.user_id.unique())) #creating a list of all unique users from dataset \n",
    "items1=list(np.sort(data1.item_id.unique())) #creating a list of all unique items from dataset\n",
    "rating1=list(data1.rating) #creating a list of all ratings from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "# creating rows and columns for our matrix\n",
    "rows1=data1.user_id.astype(int)\n",
    "cols1=data1.item_id.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a sparse matrix containing items engaged for the user with rows as user and columns as items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing a sparse matrix containing items engaged for the user with rows as user and columns as items\n",
    "data_sparse_user1=sparse.csr_matrix((rating1,(rows1, cols1)), shape=(len(users1),len(items1)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a csr matrix where rows of the matrix are the items and columns of the matrix as items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "#constructing a sparse matrix containing items engaged for the user with rows as user an\n",
    "data_sparse_item1=sparse.csr_matrix((rating1,(cols1,rows1)), shape=(len(items1),len(users1)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2239, 2174)"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse_user1.shape #shape of user_item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2174, 2239)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse_item1.shape #shape of item_user matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the datset when rating =1 means there is an interaction between user and item.\n",
    "\n",
    "When rating is 0, it can be considered as negative item that means preference (Piu)=0 and Confidence (Ciu=1) is assumed for all the negative items.\n",
    "\n",
    "Negative items can also passed with high confidence value whch can indicate that user disliked the item\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "alpha = 35 #The rate in which we'll increase our confidence in a preference with more interactions.\n",
    "data_1 = (data_sparse_item1 * alpha).astype('double')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:root:Intel MKL BLAS detected. Its highly recommend to set the environment variable 'export MKL_NUM_THREADS=1' to disable its internal multithreading\n"
     ]
    }
   ],
   "source": [
    "model1 = implicit.als.AlternatingLeastSquares(factors=5, regularization=0.15, iterations=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 30.0/30 [00:00<00:00, 196.82it/s]\n"
     ]
    }
   ],
   "source": [
    "model1.fit(data_1,show_progress=True) #fitting our data to our model."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Methodology for recommending top 10 items for each user. Same methodology for all the three models.\n",
    "\n",
    "`Steps`:\n",
    "\n",
    "   1. All the users from test data are stores `user_id`\n",
    "   \n",
    "    \n",
    "   2. A nested list `item_list` is created to store all the item corresponding to each user.\n",
    "   \n",
    "    \n",
    "   3. `.recommend()` this is an inbuilt package in implicit.als to recommend items for each user. \n",
    "    \n",
    "   Its parameters are:\n",
    "    \n",
    "   `userid`: The userid to calculate recommendations for.{stored as `user_id` in this case}\n",
    "        \n",
    "   `user_items`: A sparse matrix  of shape(number_users,numer_items).{stored as `data_sparse_user` in our case}\n",
    "        \n",
    "   `N` : Number of results to return {Size of all unique items ids in our case}\n",
    "        \n",
    "  It returns : List of itemid and corresponding score as a tuple.\n",
    "     \n",
    "   For each user we generate the set of item recommendations by giving parameters as above. Fetching only the list of  recommended item ids from the tuple for each user.\n",
    "   \n",
    "   4. From the nested loop of recommended items for each user, we compare with the item list{`item_list`} from test data and \n",
    "   \n",
    "   append only the items that meet this condition. Finally, only top 10 from this new list are taken and stored as a nested \n",
    "   \n",
    "   list `top_recommendations` where each inner list contains top 10 recommendations for each user.\n",
    "   \n",
    "     \n",
    "   5. `chain.fom_iterable` is performed on the nested lists `user_lists` and `top_recommendations` to generate a new single \n",
    "   \n",
    "   list `final users` and `top_10_recommendations`.\n",
    "   \n",
    "     \n",
    "   6. A dataframe `recommendations_dataframe` is created with above two lists where each row corresponds to a user and a \n",
    "   \n",
    "    recommended item.\n",
    "    \n",
    "     \n",
    "   7. This dataframe `recommendations_dataframe` is conveted into a csv file and used for final submissions.\n",
    "   \n",
    "   \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "user_id1=list((testData.user_id.unique())) #storing all the unique user ids as a list\n",
    "\n",
    "item_list1=[] #empty list\n",
    "for user in user_id1: #iterating through user_id list\n",
    "    item_list1.append(testData[testData['user_id']==user]['item_id'].tolist()) #storing all the item ids of each user \n",
    "\n",
    "\n",
    "\n",
    "total_items_recommend1=[] #empty list\n",
    "user_lists1=[] #empty list\n",
    "for user in user_id1: #iterating through each user id\n",
    "    top1=model1.recommend(user,data_sparse_user1,N=len(testData['item_id'].unique().tolist())) #calculates N best recommendations for a user and list of item ids and their corresponding score \n",
    "    total_items_recommend1.append([i[0]for i in top1]) #storing only item ids\n",
    "    user_lists1.append([user]*10) #10 entries of each user is created.\n",
    "\n",
    "top_recommendations1=[] # empty list\n",
    "for i in range(len(total_items_recommend1)): #iterating through length of recommendations generated for each user\n",
    "    temp=[] #empty list\n",
    "    for j in total_items_recommend1[i]: #for all the recommended items for each user\n",
    "        if j in item_list1[i]: #if that recommended item is present in item list of that user\n",
    "            temp.append(j) # append that item to new list\n",
    "            \n",
    "    top_recommendations1.append(temp[:10]) # append first 10 items satisfying the condition to a new list\n",
    "\n",
    "\n",
    "\n",
    "final_users1=list(chain.from_iterable(user_lists1)) #creating a flattened iterable of users\n",
    "top_10_recommendations1=list(chain.from_iterable(top_recommendations1)) #creating a flattened iterable of items\n",
    "\n",
    "#creating a dataframe containing final user ids and item ids\n",
    "recommendations_dataframe1=pd.DataFrame({'user_id':final_users1,'item_id':top_10_recommendations1})\n",
    "\n",
    "#writing the dataframe into a csv file\n",
    "# recommendations_dataframe1.to_csv('sample_solution_data.csv',index=False)\n",
    "recommendations_dataframe1[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Model 2\n",
    "## 2. Fitting the ALS Model with only Training Data and Validation Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iAViAKy2ag6M"
   },
   "source": [
    "# Concatenating training and validation Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "It can be noticed from shape of train dataset that its size is smaller compared to test dataset and validation dataset.\n",
    "\n",
    "It can also be inferred that trainData set merely has only 1 as rating value, hence using only this dataset wouldn't help building a generalised model and results in overfitting as the labels(ratings) of trained data is highly biased. On the other side, validation dataset has combinations of both 0's and 1's. To achieve a generalised model with lesser overfitting, both training and validation dataset are merged.\n",
    "\n",
    "Proceeding to concatenate both training dataset and validation dataset to achieve a bigger dataset and significantly get a better model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "tM4_D1Gqakzy"
   },
   "outputs": [],
   "source": [
    "data2=pd.concat([trainData,validationData],ignore_index=True) #concatenating both train and validation dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "gBM72qNIbNyF",
    "outputId": "d9faf8b7-6e11-4d0b-d228-fa3150443d56"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(252349, 3)"
      ]
     },
     "execution_count": 65,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.shape #shape of final dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "S7-cK90eLA8s"
   },
   "outputs": [],
   "source": [
    "#Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 67
    },
    "colab_type": "code",
    "id": "fz0N9BZkLA8v",
    "outputId": "88a9aea9-e9d8-4412-88c4-d688c5549537",
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0    221661\n",
       "1     30688\n",
       "Name: rating, dtype: int64"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2.rating.value_counts() #displaying value counts of ratings in dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Dropping duplicates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(247424, 3)"
      ]
     },
     "execution_count": 68,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data2 = data2.drop_duplicates()\n",
    "data2=data2.reset_index(drop=True) \n",
    "data2.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ONSqn8GtbN6S"
   },
   "outputs": [],
   "source": [
    "users2=list(np.sort(data2.user_id.unique())) #creating a list of all unique users from dataset \n",
    "items2=list(np.sort(data2.item_id.unique())) #creating a list of all unique items from dataset\n",
    "rating2=list(data2.rating) #creating a list of all ratings from dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "l3i9Vc-rbN-5"
   },
   "outputs": [],
   "source": [
    "# creating rows and columns for our matrix\n",
    "rows2=data2.user_id.astype(int)\n",
    "cols2=data2.item_id.astype(int)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a sparse matrix containing items engaged for the user with rows as user and columns as items\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "dGoNMofmbOCU"
   },
   "outputs": [],
   "source": [
    "#constructing a sparse matrix containing items engaged for the user with rows as user and columns as items\n",
    "data_sparse_user2=sparse.csr_matrix((rating2,(rows2, cols2)), shape=(len(users2),len(items2)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Constructing a csr matrix where rows of the matrix are the items and columns of the matrix as items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "kt_amkKaLA8-"
   },
   "outputs": [],
   "source": [
    "#constructing a sparse matrix containing items engaged for the user with rows as user an\n",
    "data_sparse_item2=sparse.csr_matrix((rating2,(cols2,rows2)), shape=(len(items2),len(users2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 34
    },
    "colab_type": "code",
    "id": "EbYwb7-aLA9A",
    "outputId": "8b301f3c-0fa1-4fd7-9ab8-de2f631831ad"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2239, 2174)"
      ]
     },
     "execution_count": 73,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse_user2.shape #shape of user_item matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(2174, 2239)"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_sparse_item2.shape #shape of item_user matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training the ALS Model with Combined Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|████████████████████████████████████████████████████████████████████████████████| 30.0/30 [00:00<00:00, 48.83it/s]\n"
     ]
    }
   ],
   "source": [
    "alpha = 35 #The rate in which we'll increase our confidence in a preference with more interactions.\n",
    "data_2 = (data_sparse_item2 * alpha).astype('double')\n",
    "\n",
    "model2 = implicit.als.AlternatingLeastSquares(factors=5, regularization=0.15, iterations=30)\n",
    "\n",
    "model2.fit(data_2,show_progress=True) #fitting our data to our model."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "9q8j59C0LA9m"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>150</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1217</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>555</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>383</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1766</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>1128</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>239</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>695</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>156</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>305</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id\n",
       "1         0      150\n",
       "2         0     1217\n",
       "3         0      555\n",
       "4         0      383\n",
       "5         0     1766\n",
       "6         0     1128\n",
       "7         0      239\n",
       "8         0      695\n",
       "9         0      156\n",
       "10        1      305"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id2=list((testData.user_id.unique())) #storing all the unique user ids as a list\n",
    "\n",
    "item_list2=[] #empty list\n",
    "for user in user_id2: #iterating through user_id list\n",
    "    item_list2.append(testData[testData['user_id']==user]['item_id'].tolist()) #storing all the item ids of each user \n",
    "\n",
    "\n",
    "\n",
    "total_items_recommend2=[] #empty list\n",
    "user_lists2=[] #empty list\n",
    "for user in user_id2: #iterating through each user id\n",
    "    top2=model2.recommend(user,data_sparse_user2,N=len(testData['item_id'].unique().tolist())) #calculates N best recommendations for a user and list of item ids and their corresponding score \n",
    "    total_items_recommend2.append([i[0]for i in top2]) #storing only item ids\n",
    "    user_lists2.append([user]*10) #10 entries of each user is created.\n",
    "\n",
    "top_recommendations2=[] # empty list\n",
    "for i in range(len(total_items_recommend2)): #iterating through length of recommendations generated for each user\n",
    "    temp=[] #empty list\n",
    "    for j in total_items_recommend2[i]: #for all the recommended items for each user\n",
    "        if j in item_list2[i]: #if that recommended item is present in item list of that user\n",
    "            temp.append(j) # append that item to new list\n",
    "            \n",
    "    top_recommendations2.append(temp[:10]) # append first 10 items satisfying the condition to a new list\n",
    "\n",
    "\n",
    "\n",
    "final_users2=list(chain.from_iterable(user_lists2)) #creating a flattened iterable of users\n",
    "top_10_recommendations2=list(chain.from_iterable(top_recommendations2)) #creating a flattened iterable of items\n",
    "\n",
    "#creating a dataframe containing final user ids and item ids\n",
    "recommendations_dataframe2=pd.DataFrame({'user_id':final_users2,'item_id':top_10_recommendations2})\n",
    "\n",
    "#writing the dataframe into a csv file\n",
    "recommendations_dataframe2.to_csv('sample_solution_data.csv',index=False)\n",
    "recommendations_dataframe2[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Observations :\n",
    "    \n",
    "### ALS on trainData Vs ALS on trainData + validationData\n",
    "\n",
    "There is a significant increase in accuracy when compared with model built on only train Data as the ratings are biased to 1."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model 3.\n",
    "\n",
    "# Bayesian Personalised Ranking on trainData + validationData"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████| 50/50 [00:00<00:00, 112.92it/s, correct=50.09%, skipped=5.21%]\n"
     ]
    }
   ],
   "source": [
    "# Building the Bayesian Personalized Ranking\n",
    "model3 = implicit.bpr.BayesianPersonalizedRanking(factors=5, regularization=0.1, iterations= 50)\n",
    "alpha_val = 35 # Setting the scaling parameter alpha \n",
    "data_3 = (data_sparse_item2 * alpha_val).astype('double') # Configuring the data and converting the type to double\n",
    "model3.fit(data_3) # fitting the bayesian personalized ranking model . \n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>user_id</th>\n",
       "      <th>item_id</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0</td>\n",
       "      <td>911</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0</td>\n",
       "      <td>1363</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0</td>\n",
       "      <td>1187</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0</td>\n",
       "      <td>1295</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0</td>\n",
       "      <td>1299</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>0</td>\n",
       "      <td>930</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>0</td>\n",
       "      <td>2070</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>0</td>\n",
       "      <td>1464</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>0</td>\n",
       "      <td>1196</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>1</td>\n",
       "      <td>223</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    user_id  item_id\n",
       "1         0      911\n",
       "2         0     1363\n",
       "3         0     1187\n",
       "4         0     1295\n",
       "5         0     1299\n",
       "6         0      930\n",
       "7         0     2070\n",
       "8         0     1464\n",
       "9         0     1196\n",
       "10        1      223"
      ]
     },
     "execution_count": 78,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "user_id3=list((testData.user_id.unique())) #storing all the unique user ids as a list\n",
    "\n",
    "item_list3=[] #empty list\n",
    "for user in user_id3: #iterating through user_id list\n",
    "    item_list3.append(testData[testData['user_id']==user]['item_id'].tolist()) #storing all the item ids of each user \n",
    "\n",
    "\n",
    "\n",
    "total_items_recommend3=[] #empty list\n",
    "user_lists3=[] #empty list\n",
    "for user in user_id3: #iterating through each user id\n",
    "    top3=model3.recommend(user,data_sparse_user2,N=len(testData['item_id'].unique().tolist())) #calculates N best recommendations for a user and list of item ids and their corresponding score \n",
    "    total_items_recommend3.append([i[0]for i in top3]) #storing only item ids\n",
    "    user_lists3.append([user]*10) #10 entries of each user is created.\n",
    "\n",
    "top_recommendations3=[] # empty list\n",
    "for i in range(len(total_items_recommend3)): #iterating through length of recommendations generated for each user\n",
    "    temp=[] #empty list\n",
    "    for j in total_items_recommend3[i]: #for all the recommended items for each user\n",
    "        if j in item_list3[i]: #if that recommended item is present in item list of that user\n",
    "            temp.append(j) # append that item to new list\n",
    "            \n",
    "    top_recommendations3.append(temp[:10]) # append first 10 items satisfying the condition to a new list\n",
    "\n",
    "\n",
    "\n",
    "final_users3=list(chain.from_iterable(user_lists3)) #creating a flattened iterable of users\n",
    "top_10_recommendations3=list(chain.from_iterable(top_recommendations3)) #creating a flattened iterable of items\n",
    "\n",
    "#creating a dataframe containing final user ids and item ids\n",
    "recommendations_dataframe3=pd.DataFrame({'user_id':final_users3,'item_id':top_10_recommendations3})\n",
    "\n",
    "#writing the dataframe into a csv file\n",
    "# recommendations_dataframe3.to_csv('sample_solution_data.csv',index=False)\n",
    "recommendations_dataframe3[1:11]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Achieved accuracy for BPR is which is significantly low. ALS algorithm is a better choice in this case.\n",
    "\n",
    "Main advantages of ALS are :\n",
    "1. Implicit datasets are usually sprase and ALS optimisation technique is more efficient approach compared to other techniques such as SGD.\n",
    "2. It is very easy to parallelize."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above three models, ALS tuned(trainData+ValidationData) model achieved highest score among all and henceforth used for final kaggle submission."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "nbcB3miarwaP"
   },
   "source": [
    "# Task 2\n",
    "\n",
    "# Node Classification"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Importing the packages required for this task\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import networkx as nx\n",
    "from node2vec import Node2Vec\n",
    "from gensim.models import Word2Vec  #word2vec\n",
    "from sklearn.metrics import precision_score, recall_score, f1_score, accuracy_score, confusion_matrix, matthews_corrcoef\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.naive_bayes import BernoulliNB\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from nltk.corpus import stopwords\n",
    "import nltk\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "About Datasets:\n",
    "\n",
    "1. `docs.txt` : Contains title information for rach node ID\n",
    "\n",
    "\n",
    "2. `adjedges.txt` : Contains neighbour nodes of each node in a network. In each row, first item is a node ID and the rest items are nodes that are linked to the first item.\n",
    "\n",
    "\n",
    "3. `labels.txt` : Contains class labels for each node. Each row represents a node id and its corresponding class label."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read graph in adjacency list format from path."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "G=nx.read_adjlist(\"adjedges.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node2Vec\n",
    "\n",
    "* In Node2Vec we learn to map nodes to a low dimensional space of features that maximises the likelihood of preserving network neighbourhood of nodes.\n",
    "\n",
    "\n",
    "* In node classification task, we are interested in predicting most probable labels of nodes in a network.\n",
    "\n",
    "\n",
    "* In prediction problem on network, one has to construct a feature vector representation of edges and nodes.\n",
    "\n",
    "\n",
    "* While this supervised procedure results in good accuracy, it comes at the cost of high training time complexity due to a blow up in the number of parameters that need to be estimated.\n",
    "\n",
    "\n",
    "* Algorithm should learn node representationss that embed nodes from the same network community close to each other. And also nodes that share similar roles have similar embeddings. This would allow feature learning algorithms to generalise across a wide variety of domains and prediction tasks.\n",
    "\n",
    "\n",
    "* In summary, `Node2vec` is a semi-supervised learning algorithm for scalable feature learning in networks. By choosing an appropriate notion of a neighborhood, node2vec can learn representations that organise the nodes based on their network roles. We achieve this by developing a family of biased random walks, which efficiently explore neighborhoods of given node."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Initialising `Node2vec` on the citation network  with the following parameters.\n",
    "1. `walk_length` : Number of nodes the algorithm visits in each walk\n",
    "2. `num_walk` : Number of times the algorithm performs the random walks\n",
    "3. `dimensions` : dimensions of the vector generated by the algorithm\n",
    "4. `workers` : number of workers the algorithm uses for parallelisation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Computing transition probabilities: 100%|██████████████████████████████████████| 36928/36928 [00:11<00:00, 3131.88it/s]\n"
     ]
    }
   ],
   "source": [
    "# pre-compute the probabilities and generate walks :\n",
    "node2vec = Node2Vec(G, dimensions=15, walk_length=30, num_walks=20,workers=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, `node2vec.fit` uses the vector generated by node2vec algorithm and runs word2vec algorithm on top of it\n",
    "Its parameters are :\n",
    "1. `window` : defines total number of words before and after current word used by word2vec algorithm\n",
    "2. `min_count` : frequency of words that need to be dropped \n",
    "3. `batch_words` : size of chunk sent to each workermm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "#embed the nodes\n",
    "\n",
    "model = node2vec.fit(window=10, min_count=1, batch_words=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importing pickle for saving the model and its reusability.\n",
    "# import pickle\n",
    "\n",
    "\n",
    "# pickle_out = open(\"dict4.pickle\",\"wb\")\n",
    "# pickle.dump(model, pickle_out)\n",
    "# pickle_out.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# pickle_in = open(\"dict3.pickle\",\"rb\")\n",
    "# model= pickle.load(pickle_in)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Reading labels dataset  and doing some pre processing to create a final dataframe with two columns i.e. nodes and their corresponding labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>First</th>\n",
       "      <th>Last</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12828558</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66779408</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38902949</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33450563</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57470294</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      First Last\n",
       "0  12828558    0\n",
       "1  66779408    0\n",
       "2  38902949    0\n",
       "3  33450563    0\n",
       "4  57470294    0"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "labels=pd.read_csv('labels.txt',header=None) #reading labels dataset\n",
    "\n",
    "labels.columns=['Name'] #giving the name of column \n",
    "\n",
    "labels[['First','Last']] = labels.Name.str.split(\" \",expand=True,) #splitting the column and storing as two columns where one column contains node ID and other column contains its correspoding labels\n",
    "\n",
    "del labels['Name'] #deleting the previous name column\n",
    "\n",
    "labels.head() # looking at first 5 rows"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using the generated model and constructing the vectorised form of each node  and storing all node vectors as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "X=model.wv[labels['First']].tolist()  #generating and storing node vectors as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing all their corresponding labels as a list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "y=(labels['Last'].tolist()) #storing labels as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the final data (that contains node vectors and their corresponding labels for each node ID) into train, test split set with stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X, y, test_size=0.8,shuffle=True,stratify=y)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing and evaluating the performance of different classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using model : Logistic Regression on Node Embedding Vectors is :0.49432425213675213\n",
      "Accuracy achieved by using model : Bernoulli NB on Node Embedding Vectors is :0.36658653846153844\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using model : Linear SVC on Node Embedding Vectors is :0.49659455128205127\n",
      "Accuracy achieved by using model : Random Forest Classifier on Node Embedding Vectors is :0.5406650641025641\n"
     ]
    }
   ],
   "source": [
    "accuracy_list=[]\n",
    "models_list=[]\n",
    "embedding_list=[]\n",
    "models = [LogisticRegression(),BernoulliNB(),LinearSVC(),RandomForestClassifier()] #storing each model as list\n",
    "classifiers=['Logistic Regression','Bernoulli NB','Linear SVC','Random Forest Classifier'] #corresponding model names\n",
    "for i in range(len(models)): #looping over each model from model list\n",
    "    clf=models[i] # taking each model\n",
    "    clf.fit(x_train,y_train) #fitting train data on each model\n",
    "    y_predict=clf.predict(x_test) #making predictions on test set\n",
    "    accuracy=accuracy_score(y_test,y_predict) #calculating the accuracies\n",
    "    print('Accuracy achieved by using model : '+classifiers[i]+' on Node Embedding Vectors is :'+str(accuracy)) #printing model and its accuracy\n",
    "    accuracy_list.append(accuracy)\n",
    "    models_list.append(classifiers[i])\n",
    "    embedding_list.append('Node Embedding')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### By doing Node Embedding, among the above 4 classifiers, accuracy is comparitively high for Linear SVC(). Its accuracy is around 0.55.\n",
    "\n",
    "#### But, Accuracy achieved seems to be very less when node embedding is performed. Let's leverage the \n",
    "####  `docs.txt` file to see if there is any improvement in the accuracy\n",
    " \n",
    "#### This filecontains title information of each node and convert into vectors in latent space.\n",
    "\n",
    "\n",
    "#### Generate vectors for each node ID by doing Text embedding and seeing if there is any improvement in accuracy\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Word2Vec Text Embedding\n",
    "\n",
    "## Reading the docs file and doing some preprocessing to perform further operations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\ipykernel_launcher.py:1: ParserWarning: Falling back to the 'python' engine because the 'c' engine does not support regex separators (separators > 1 char and different from '\\s+' are interpreted as regex); you can avoid this warning by specifying engine='python'.\n",
      "  \"\"\"Entry point for launching an IPython kernel.\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Node ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12828558</td>\n",
       "      <td>Assessing Local Institutional Capacity, Data A...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66779408</td>\n",
       "      <td>THE PROSPECTS FOR INTERNET TELEPHONY IN EUROPE...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38902949</td>\n",
       "      <td>Economic Shocks, Safety Nets, and Fiscal Const...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33450563</td>\n",
       "      <td>Reform, Growth, and Poverty in Vietnam</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57470294</td>\n",
       "      <td>Households and Economic Growth in Latin Americ...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Node ID                                               text\n",
       "0  12828558  Assessing Local Institutional Capacity, Data A...\n",
       "1  66779408  THE PROSPECTS FOR INTERNET TELEPHONY IN EUROPE...\n",
       "2  38902949  Economic Shocks, Safety Nets, and Fiscal Const...\n",
       "3  33450563             Reform, Growth, and Poverty in Vietnam\n",
       "4  57470294  Households and Economic Growth in Latin Americ..."
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs=pd.read_csv('docs.txt',header=None,sep=\" \\n\") # reading the text file as dataframe\n",
    "\n",
    "docs.columns=['Name'] #assigning the column name\n",
    "\n",
    "docs[['Node ID','text']] = docs.Name.str.split(n=1,expand=True) #splitting the column into two columns that contains node id and text\n",
    "\n",
    "del docs['Name']\n",
    "\n",
    "docs.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18720, 2)"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.shape #shape of the file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def clean_text(text):\n",
    "    \"\"\"\n",
    "    :param X: a text\n",
    "    :return: clean tokens of the text\n",
    "    \"\"\"\n",
    "    text=text.lower() # converting to lower case\n",
    "    tokens= nltk.word_tokenize(text) #building tokens\n",
    "    tokens=[x for x in tokens if x.isalpha()] #taking only alphabets\n",
    "    tokens= [word for word in tokens if word not in stopwords.words('english')] #removing stop words\n",
    "    return tokens #returning the tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['text']=docs['text'].apply(clean_text) # cleaning the text and converting into tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Node ID</th>\n",
       "      <th>text</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>12828558</td>\n",
       "      <td>[assessing, local, institutional, capacity, da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>66779408</td>\n",
       "      <td>[prospects, internet, telephony, europe, latin...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>38902949</td>\n",
       "      <td>[economic, shocks, safety, nets, fiscal, const...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>33450563</td>\n",
       "      <td>[reform, growth, poverty, vietnam]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>57470294</td>\n",
       "      <td>[households, economic, growth, latin, america,...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Node ID                                               text\n",
       "0  12828558  [assessing, local, institutional, capacity, da...\n",
       "1  66779408  [prospects, internet, telephony, europe, latin...\n",
       "2  38902949  [economic, shocks, safety, nets, fiscal, const...\n",
       "3  33450563                 [reform, growth, poverty, vietnam]\n",
       "4  57470294  [households, economic, growth, latin, america,..."
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.head() #first 5 rows"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_word=  Word2Vec(docs.text,min_count=1,sg=1) # building the Word2Vec model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 0.25996387,  0.17589825,  0.12277557,  0.0102542 ,  0.19415604,\n",
       "        0.23257054,  0.43391833,  0.12421094, -0.18527009,  0.10487337,\n",
       "        0.05872114, -0.18760754, -0.09540234,  0.3163379 , -0.03498618,\n",
       "       -0.03432474, -0.20649676,  0.0178138 ,  0.08232713, -0.06239962,\n",
       "        0.04188103,  0.19754095, -0.05260567, -0.00089449, -0.09756522,\n",
       "        0.12079086,  0.01768522,  0.00132022,  0.00126887,  0.5098676 ,\n",
       "       -0.26902422, -0.09469157,  0.23773555,  0.3465759 , -0.06089936,\n",
       "       -0.16235207,  0.30857548, -0.08242504, -0.26155677, -0.0211451 ,\n",
       "        0.33333912, -0.35047138,  0.02982399, -0.21133617,  0.09423383,\n",
       "       -0.37120828, -0.07634153, -0.03794492,  0.11051329, -0.10020666,\n",
       "        0.0374297 , -0.0320542 , -0.3782241 ,  0.11657822,  0.04436144,\n",
       "       -0.07858814,  0.03941964, -0.4391593 , -0.07736164,  0.12433764,\n",
       "        0.16326521, -0.0962167 ,  0.18866411,  0.20158297,  0.42449316,\n",
       "       -0.23560716,  0.22264405, -0.06178874, -0.29264942, -0.21660551,\n",
       "        0.35834885,  0.04435223,  0.12174328,  0.15746197, -0.33966392,\n",
       "       -0.13665518,  0.07018074,  0.00180248,  0.23853388, -0.04787809,\n",
       "       -0.05714159,  0.3053295 ,  0.13968287,  0.02731486, -0.12037103,\n",
       "        0.18556264, -0.07539371, -0.07447597, -0.05316611,  0.22043398,\n",
       "       -0.00989233,  0.15977773,  0.13442852,  0.14462812,  0.35138994,\n",
       "        0.00340332,  0.00401959,  0.08342247, -0.18959956, -0.10577767],\n",
       "      dtype=float32)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#vector representaion of one of the words.\n",
    "model_word.wv['assessing']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating a new column and and storing the node vectors for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['node_vectors']=X #storing node vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Creating a new column and storing the labels for each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "docs['y']=y #storing node labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18720, 4)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "docs.shape  #shape of the final dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Checking for rows if there are any empty tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Number of rows that have empty tokens are : 218\n"
     ]
    }
   ],
   "source": [
    "indexes=[] #empty rows\n",
    "for i in range(len(docs['text'])): #looping over text rows\n",
    "    if len(docs['text'][i])==0: #if the length of token list is zero\n",
    "        indexes.append(i) #appending those indexes\n",
    "print('Number of rows that have empty tokens are :',len(indexes))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Deleting all the rows that have empty tokens in tokens column i.e. 'text'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(18502, 4)"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "doc=docs[docs['text'].map(len) !=0]  # removing the rows with empty tokens\n",
    "\n",
    "doc.shape #shape of the new dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### It can be seen that around 200 rows are dropped."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc=doc.reset_index(drop=True) #resetting the index"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Replacing all the tokens with their corresponding vectors generated using Word2Vec and storing it as a new column in the data frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc['text_vectors']= doc['text'].apply(lambda x: [model_word.wv[word] for word in x])# Replacing all the tokens with their corresponding vectors generated using Word2Vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Adding up all the vectors to achieve a summed vector that represents each node"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc['text_vector_sum']=doc['text_vectors'].apply(lambda x: sum(x)) #adding up all the vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing all the text vectors as a list and their corresponding labels as lists which are further used for model building and calculate accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_text=doc['text_vector_sum'].tolist() #taking this vector column as a list\n",
    "Y_text=doc['y'].tolist() #taking the l=final labels as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the final data (that contains text vectors and their corresponding labels for each node ID) into train, test split set with stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_text, Y_text, test_size=0.8,shuffle=True,stratify=Y_text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using model : Logistic Regression on Text Embedding Vectors is :0.6791649777057155\n",
      "Accuracy achieved by using model : Bernoulli NB on Text Embedding Vectors is :0.5366842318605594\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using model : Linear SVC on Text Embedding Vectors is :0.6942305093906229\n",
      "Accuracy achieved by using model : Random Forest Classifier on Text Embedding Vectors is :0.6375489798675854\n"
     ]
    }
   ],
   "source": [
    "models = [LogisticRegression(),BernoulliNB(),LinearSVC(),RandomForestClassifier()] #storing each model as list\n",
    "classifiers=['Logistic Regression','Bernoulli NB','Linear SVC','Random Forest Classifier'] #corresponding model names\n",
    "\n",
    "for i in range(len(models)): #looping over the models \n",
    "    clf=models[i] #taking each model\n",
    "    clf.fit(x_train,y_train) #fitting train dataset\n",
    "    y_predict=clf.predict(x_test) #making predictions on test data\n",
    "    accuracy=accuracy_score(y_test,y_predict) #calculating the accuracy\n",
    "    print('Accuracy achieved by using model : '+classifiers[i]+' on Text Embedding Vectors is :'+str(accuracy)) #printing the model and its corresponding accuracy\n",
    "    accuracy_list.append(accuracy)\n",
    "    models_list.append(classifiers[i])\n",
    "    embedding_list.append('Text Embedding')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is an improvement in the accuracy compared to Node Embedding. Linear SVC() accuracy achieved by doing text embedding is around 70 %"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Node embedding + Text Embedding\n",
    "\n",
    "### Concatenating vectors generated through node embedding and text embedding for each node ID and use this final vector for model development and see if there is an improvement in accuracy.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "final=[] #empty list\n",
    "for i in range(len(doc['node_vectors'])): #iterating over length of dataframe column\n",
    "    final.append(np.array(doc['text_vector_sum'][i].tolist()+doc['node_vectors'][i])) #concatenating both node vectors and text vectors and appending to a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing this concatenated vector list as a column of dataframe. This column represents possible vector representation of each node in latent space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "doc['node_text']=final #storing this concatenated vector list as a column of dataframe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Storing all the concatenated node+text vectors as a list and their corresponding labels as lists which are further used for model building and calculate accuracies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_final=doc['node_text'].tolist() #taking this vector column as a list\n",
    "\n",
    "Y_final=doc['y'].tolist() #taking the l=final labels as a list"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Splitting the final data (that contains concatenated node+text vectors and their corresponding labels for each node ID) into train, test split set with stratified sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_train, x_test, y_train, y_test = train_test_split(X_final, Y_final, test_size=0.8,shuffle=True,stratify=Y_final)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Comparing and evaluating the performance of different classification algorithms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:432: FutureWarning: Default solver will be changed to 'lbfgs' in 0.22. Specify a solver to silence this warning.\n",
      "  FutureWarning)\n",
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\sklearn\\linear_model\\logistic.py:469: FutureWarning: Default multi_class will be changed to 'auto' in 0.22. Specify the multi_class option to silence this warning.\n",
      "  \"this warning.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using model : Logistic Regression on Concatenated Node + Text Vectors is :0.7286853127955681\n",
      "Accuracy achieved by using model : Bernoulli NB on Concatenated Node + Text Vectors is :0.5535738413727874\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\sklearn\\svm\\base.py:929: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n",
      "C:\\Users\\saite\\Anaconda3\\lib\\site-packages\\sklearn\\ensemble\\forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
      "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy achieved by using model : Linear SVC on Concatenated Node + Text Vectors is :0.7407107147682745\n",
      "Accuracy achieved by using model : Random Forest Classifier on Concatenated Node + Text Vectors is :0.6876773408998784\n"
     ]
    }
   ],
   "source": [
    "models = [LogisticRegression(),BernoulliNB(),LinearSVC(),RandomForestClassifier()] #storing each model as list\n",
    "classifiers=['Logistic Regression','Bernoulli NB','Linear SVC','Random Forest Classifier'] #corresponding model names\n",
    "\n",
    "for i in range(len(models)): #looping over the models \n",
    "    clf=models[i] #taking each model\n",
    "    clf.fit(x_train,y_train) #fitting train dataset\n",
    "    y_predict=clf.predict(x_test) #making predictions on test data\n",
    "    accuracy=accuracy_score(y_test,y_predict) #calculating the accuracy\n",
    "    print('Accuracy achieved by using model : '+classifiers[i]+' on Concatenated Node + Text Vectors is :'+str(accuracy)) #printing the model and its corresponding accuracy\n",
    "    accuracy_list.append(accuracy)\n",
    "    models_list.append(classifiers[i])\n",
    "    embedding_list.append('Node + Text')    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### There is an improvement in the accuracy compared to Node Embedding. Linear SVC() accuracy achieved by concatenating Node + Text embedding is around 75 %\n",
    "\n",
    "### Constructing a Data Frame to compare Embedding Types and their accuracies on various models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "table={'Model Name':models_list,'Embedding Type':embedding_list,'Accuracy':accuracy_list} #creating a dictionary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "accuracy_dataframe=pd.DataFrame(table) # converting dictionary to dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Embedding Type</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Node Embedding</td>\n",
       "      <td>0.494324</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bernoulli NB</td>\n",
       "      <td>Node Embedding</td>\n",
       "      <td>0.366587</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>Node Embedding</td>\n",
       "      <td>0.496595</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>Node Embedding</td>\n",
       "      <td>0.540665</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Text Embedding</td>\n",
       "      <td>0.679165</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>Bernoulli NB</td>\n",
       "      <td>Text Embedding</td>\n",
       "      <td>0.536684</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>Text Embedding</td>\n",
       "      <td>0.694231</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>Text Embedding</td>\n",
       "      <td>0.637549</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>Logistic Regression</td>\n",
       "      <td>Node + Text</td>\n",
       "      <td>0.728685</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>9</th>\n",
       "      <td>Bernoulli NB</td>\n",
       "      <td>Node + Text</td>\n",
       "      <td>0.553574</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>Node + Text</td>\n",
       "      <td>0.740711</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>11</th>\n",
       "      <td>Random Forest Classifier</td>\n",
       "      <td>Node + Text</td>\n",
       "      <td>0.687677</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                  Model Name  Embedding Type  Accuracy\n",
       "0        Logistic Regression  Node Embedding  0.494324\n",
       "1               Bernoulli NB  Node Embedding  0.366587\n",
       "2                 Linear SVC  Node Embedding  0.496595\n",
       "3   Random Forest Classifier  Node Embedding  0.540665\n",
       "4        Logistic Regression  Text Embedding  0.679165\n",
       "5               Bernoulli NB  Text Embedding  0.536684\n",
       "6                 Linear SVC  Text Embedding  0.694231\n",
       "7   Random Forest Classifier  Text Embedding  0.637549\n",
       "8        Logistic Regression     Node + Text  0.728685\n",
       "9               Bernoulli NB     Node + Text  0.553574\n",
       "10                Linear SVC     Node + Text  0.740711\n",
       "11  Random Forest Classifier     Node + Text  0.687677"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dataframe # displaying the dataframe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Model Name</th>\n",
       "      <th>Embedding Type</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>10</th>\n",
       "      <td>Linear SVC</td>\n",
       "      <td>Node + Text</td>\n",
       "      <td>0.740711</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "    Model Name Embedding Type  Accuracy\n",
       "10  Linear SVC    Node + Text  0.740711"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "accuracy_dataframe[accuracy_dataframe.Accuracy==accuracy_dataframe['Accuracy'].max()]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### From the above dataframe it can be seen that Linear SVC() performed better than other three classifier in terms of accuracy and it can be noticed that highest accuracy is achieved on a model where data was from combining both Node vectors and Text vectors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#the End"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "name": "Semi_Structured_Assignment.ipynb",
   "provenance": [],
   "toc_visible": true
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "0cb84117f20f481489212f9a2fc2d604": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "FloatProgressModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "FloatProgressModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "ProgressView",
      "bar_style": "success",
      "description": "100%",
      "description_tooltip": null,
      "layout": "IPY_MODEL_5d380cf8dab0494e9ff3f92b76334d49",
      "max": 50,
      "min": 0,
      "orientation": "horizontal",
      "style": "IPY_MODEL_ceed686cb82b4912bfab8fa364c01275",
      "value": 50
     }
    },
    "1a922eb98b324e058a79d36321d04071": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "3d891f68ecd846399181977d0cb2511f": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HBoxModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HBoxModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HBoxView",
      "box_style": "",
      "children": [
       "IPY_MODEL_0cb84117f20f481489212f9a2fc2d604",
       "IPY_MODEL_5b052c091a9544a48e6dc51397ebdd33"
      ],
      "layout": "IPY_MODEL_44c4534debb045918602c1788a3269c9"
     }
    },
    "44c4534debb045918602c1788a3269c9": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "5b052c091a9544a48e6dc51397ebdd33": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "HTMLModel",
     "state": {
      "_dom_classes": [],
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "HTMLModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/controls",
      "_view_module_version": "1.5.0",
      "_view_name": "HTMLView",
      "description": "",
      "description_tooltip": null,
      "layout": "IPY_MODEL_1a922eb98b324e058a79d36321d04071",
      "placeholder": "​",
      "style": "IPY_MODEL_6be605465f04475f98c05918a46192ac",
      "value": " 50/50 [00:02&lt;00:00, 21.94it/s]"
     }
    },
    "5d380cf8dab0494e9ff3f92b76334d49": {
     "model_module": "@jupyter-widgets/base",
     "model_name": "LayoutModel",
     "state": {
      "_model_module": "@jupyter-widgets/base",
      "_model_module_version": "1.2.0",
      "_model_name": "LayoutModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "LayoutView",
      "align_content": null,
      "align_items": null,
      "align_self": null,
      "border": null,
      "bottom": null,
      "display": null,
      "flex": null,
      "flex_flow": null,
      "grid_area": null,
      "grid_auto_columns": null,
      "grid_auto_flow": null,
      "grid_auto_rows": null,
      "grid_column": null,
      "grid_gap": null,
      "grid_row": null,
      "grid_template_areas": null,
      "grid_template_columns": null,
      "grid_template_rows": null,
      "height": null,
      "justify_content": null,
      "justify_items": null,
      "left": null,
      "margin": null,
      "max_height": null,
      "max_width": null,
      "min_height": null,
      "min_width": null,
      "object_fit": null,
      "object_position": null,
      "order": null,
      "overflow": null,
      "overflow_x": null,
      "overflow_y": null,
      "padding": null,
      "right": null,
      "top": null,
      "visibility": null,
      "width": null
     }
    },
    "6be605465f04475f98c05918a46192ac": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "DescriptionStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "DescriptionStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "description_width": ""
     }
    },
    "ceed686cb82b4912bfab8fa364c01275": {
     "model_module": "@jupyter-widgets/controls",
     "model_name": "ProgressStyleModel",
     "state": {
      "_model_module": "@jupyter-widgets/controls",
      "_model_module_version": "1.5.0",
      "_model_name": "ProgressStyleModel",
      "_view_count": null,
      "_view_module": "@jupyter-widgets/base",
      "_view_module_version": "1.2.0",
      "_view_name": "StyleView",
      "bar_color": null,
      "description_width": "initial"
     }
    }
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
